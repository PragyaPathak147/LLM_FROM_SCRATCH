# Building LLMs From Scratch ğŸ§ ğŸ“š

This project demonstrates the foundational concepts behind building a Transformer-based Language Model from scratch, inspired by GPT-like architectures. It includes end-to-end components such as tokenization, positional encoding, multi-head self-attention, feedforward layers, and training loop on a small text corpus.

## ğŸš€ Features

- Byte Pair Encoding (BPE) tokenizer using `tiktoken`
- Custom Transformer Encoder implementation
- Multi-Head Attention, Layer Normalization, Residual Connections
- Positional Encoding
- Trained on a Harry Potter subset (~12M parameters)
- Next-token prediction objective
- PyTorch-based model architecture

## ğŸ› ï¸ Project Structure
â”œâ”€â”€ sample_corpus/ # Contains sample text (Harry Potter subset)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

bash
Copy
Edit

## ğŸ”§ Setup

Clone the repo and install dependencies:

```bash
git clone https://github.com/yourusername/llm-from-scratch.git
cd llm-from-scratch
pip install -r requirements.txt
